{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ddb2b7b-b2d4-4033-87e2-c26f163c7fdc",
   "metadata": {},
   "source": [
    "### <center>Start of Modelling</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac49463-e1ee-4767-81bb-a2c915847178",
   "metadata": {},
   "source": [
    "### 6.2.0. Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "284853cb-3735-4ba4-b84e-5f54f73b3629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis and Wrangling Packages\n",
      "- pandas version: 2.1.4\n",
      "****************************************\n",
      "Modelling Packages\n",
      "- scikit-learn version: 1.3.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "def separate():\n",
    "    print('*' * 40)\n",
    "\n",
    "print(\"Data Analysis and Wrangling Packages\")\n",
    "import pandas as pd # Library for data processing and analysis.\n",
    "print(\"- pandas version: {}\". format(pd.__version__))\n",
    "separate()\n",
    "\n",
    "print(\"Modelling Packages\")\n",
    "import sklearn # Collection of machine learning algorithms.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "import joblib # To export model.\n",
    "print(\"- scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cluster import AffinityPropagation, Birch, DBSCAN, KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "separate()\n",
    "\n",
    "# Ignore depreciation warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "# Track time taken to run (for modelling).\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c210223-0534-465a-a569-29a0dff44af6",
   "metadata": {},
   "source": [
    "### 6.2.1. Modelling Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d4a931-20cf-40dc-840f-2382c8ac57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquiring data.\n",
    "adult_data = pd.read_csv(\"./data/adult_processed.csv\") \n",
    "\n",
    "# X consists of all the features (target is excluded).\n",
    "X = adult_data.drop([\"income\"], axis=1) \n",
    "\n",
    "# y consists of the target column `income`.\n",
    "y = adult_data[\"income\"] \n",
    "\n",
    "# Splitting the data 7:3.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state = 2024) \n",
    "\n",
    "# All 17 machine learning algorithms.\n",
    "model_list = [\n",
    "    (\"BIRCH\", Birch()),\n",
    "    (\"Perceptron\", Perceptron()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Neural Network\", MLPClassifier()),\n",
    "    (\"Adaptive Boosting\", AdaBoostClassifier()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"K-Means Clustering\", KMeans()),\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"K-Nearest Neighbours\", KNeighborsClassifier()),\n",
    "    (\"Affinity Propagation\", AffinityPropagation()),\n",
    "    (\"Gaussian Naive Bayes\", GaussianNB()), \n",
    "    (\"Linear Support Vector\", LinearSVC()),  \n",
    "    (\"Bagging Classification\", BaggingClassifier()),\n",
    "    (\"Support Vector Classifier\", SVC()),\n",
    "    (\"Stochastic Gradient Descent\", SGDClassifier()),\n",
    "    (\"Passive Aggressive Classifier\", PassiveAggressiveClassifier()) \n",
    "]\n",
    "\n",
    "# List to store all performances of the models.\n",
    "results = []\n",
    "\n",
    "# Store the name of each model for visualization purposes.\n",
    "model_names = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fa85a-faf9-4017-9ce9-2ab1419fc5c3",
   "metadata": {},
   "source": [
    "### 6.2.2. Training the 17 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf569cf3-42dd-482b-a3fe-421d54a9cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Time taken: 6830.963931798935 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "for name, model in model_list:\n",
    "    # Cross-validation, 5 folds.\n",
    "    kfold = KFold(n_splits=5) \n",
    "    # Standard of measure: \"accuracy\" (suitable for this classification problem).\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy') \n",
    "    # Append the mean of 5 folds for each model.\n",
    "    results.append(round(cv_results.mean() * 100, 2)) \n",
    "    # Attach the model name.\n",
    "    model_names.append(name) \n",
    "\n",
    "# Calculate time taken.\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Indication when training is complete.\n",
    "print(f\"Training complete. Time taken: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ceaae9-e366-4035-a2b2-c71dab20a0f0",
   "metadata": {},
   "source": [
    "### 6.2.3. Scores and Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a185a457-dec3-4b2d-87cb-a9e6eaac887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>84.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adaptive Boosting</td>\n",
       "      <td>84.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>84.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>83.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Linear Support Vector</td>\n",
       "      <td>83.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>83.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>83.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>82.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bagging Classification</td>\n",
       "      <td>82.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>K-Nearest Neighbours</td>\n",
       "      <td>82.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>82.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>81.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Passive Aggressive Classifier</td>\n",
       "      <td>79.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>76.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIRCH</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K-Means Clustering</td>\n",
       "      <td>15.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Affinity Propagation</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  Score\n",
       "6               Gradient Boosting  84.26\n",
       "5               Adaptive Boosting  84.21\n",
       "4                  Neural Network  84.04\n",
       "14      Support Vector Classifier  83.91\n",
       "12          Linear Support Vector  83.61\n",
       "8             Logistic Regression  83.58\n",
       "2                   Random Forest  83.14\n",
       "15    Stochastic Gradient Descent  82.85\n",
       "13         Bagging Classification  82.75\n",
       "9            K-Nearest Neighbours  82.05\n",
       "3                   Decision Tree  82.03\n",
       "11           Gaussian Naive Bayes  81.05\n",
       "16  Passive Aggressive Classifier  79.17\n",
       "1                      Perceptron  76.43\n",
       "0                           BIRCH  40.27\n",
       "7              K-Means Clustering  15.04\n",
       "10           Affinity Propagation   0.22"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models = pd.DataFrame({\n",
    "    \"Model\": model_names,\n",
    "    \"Score\": results})\n",
    "compare_models.sort_values(by=\"Score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336d70c-ba21-4538-8078-c60f99bc7f28",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- Out of 17 models tested, 12 achieved an accuracy of at least 80% on our binary classification task, which is considered exemplary.\n",
    "- Although the Perceptron and Passive Aggressive Classifier performed well, achieving accuracy scores in the 70s range, they fell slightly short of the 80% mark.\n",
    "- The BIRCH model's accuracy of 40% indicates that it technically performs worse than a coin-flip, although it's worth noting that the binary target distribution tends to be skewed toward 0 rather than 1.\n",
    "- The K-Means Clustering and Affinity Propagation models yielded particularly low accuracies, suggesting that these models are not suitable for our specific problem and dataset.\n",
    "\n",
    "#### Due to resource constraints, we will only conduct GridSearch on the best-performing model: Gradient Boosting.\n",
    "\r\n",
    "Note: While we are using GridSearch, the number of iterations will be limited to approximately 1000. Consider this a proof-of-concept, but we will select appropriate parameters and ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb01bfd-06e2-472f-b996-ea4f26ed35d2",
   "metadata": {},
   "source": [
    "### 6.3.1. GridSearch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01127bb-3164-45b7-932d-0f9f7e91e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best performing algorithm. \n",
    "gbc = GradientBoostingClassifier(random_state=2048) \n",
    "\n",
    "param_grid = { \n",
    "    \"n_estimators\": [100, 300, 500, 700],  # Number of trees in the forest (higher will increase accuracy, but risk overfitting).\n",
    "    \"learning_rate\": [0.5, 0.25, 0.1, 0.05],  # Learning rate (lower requires more trees to capture data relationships but can help prevent overfitting\n",
    "    \"max_depth\": [3, 5, 7, 10],  # Maximum depth of the trees (Deeper trees can capture more complex patterns but are also more prone to overfitting).\n",
    "    \"min_samples_split\": [1, 2, 3, 5]  # Minimum number of samples required to split a node (higher results in simpler trees and  less overfitting).\n",
    "}\n",
    "\n",
    "# Number of iterations: 4 (n_estimators) * 4 (learning_rate) * 4 (max_depth) * 4 (min_samples_split) * 4 (CV) = 1024 iterations.\n",
    "grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe0aed-affa-44e6-bdfe-ea744ce2c89f",
   "metadata": {},
   "source": [
    "### 6.3.2. Running GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dfef0b7-016a-46b4-9506-f9ef4a2b9438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Time taken: 1539.84135556221 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform GridSearch.\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Calculate time taken.\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Indication when training is complete.\n",
    "print(f\"Training complete. Time taken: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf88a1f-cf35-4d82-ae70-c8b291268a1f",
   "metadata": {},
   "source": [
    "### 6.3.3. GridSearch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6084008f-a6d5-4b90-b497-1ebaf5ebda9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'learning_rate': 0.05, 'max_depth': 7, 'min_samples_split': 3, 'n_estimators': 100}\n",
      "Accuracy:  0.8458393192617969\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117e2ce-1d2c-499a-a7b2-e9e97ab7864a",
   "metadata": {},
   "source": [
    "#### Through GridSearch, we identified the best-performing parameter combination.\r\n",
    "\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66c04b-0795-4217-b3f7-480fe5b0ad4f",
   "metadata": {},
   "source": [
    "### 6.4. Optimal Parameters vs Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a747d978-844c-4282-a146-7b742ce9fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:  84.66\n",
      "GridSearch-ed Parameters:  86.08\n"
     ]
    }
   ],
   "source": [
    "# Train a new base Gradient Boosting Model.\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, Y_train)\n",
    "acc_gbc = round(gbc.score(X_train, Y_train) * 100, 2)\n",
    "print(\"Base Model: \", acc_gbc)\n",
    "\n",
    "\n",
    "# Train a new Gradient Boosting Model with the best parameters from GridSearchCV.\n",
    "gbc_gs = GradientBoostingClassifier(learning_rate=0.05, max_depth=7, min_samples_split=3, n_estimators=100)\n",
    "gbc_gs.fit(X_train, Y_train)\n",
    "acc_gbc_gs = round(gbc_gs.score(X_train, Y_train) * 100, 2)\n",
    "print(\"GridSearch-ed Parameters: \", acc_gbc_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934c746-42d0-4034-9df9-7c2bed397062",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "- The base model already performs exceptionally well with an accuracy of 84%. \n",
    "- After implementing GridSearch (although not exhaustively), we managed to improve the accuracy further, achieving 86%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee6603-8c76-43cc-aa8b-c019d09945fa",
   "metadata": {},
   "source": [
    "### 6.5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22104072-31a6-4c08-a3d9-4d29716d7528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importance:\n",
      "\n",
      "relationship    - 41.69%\n",
      "education_num   - 19.42%\n",
      "capital-gain    - 12.11%\n",
      "occupation      - 6.90%\n",
      "age             - 6.35%\n",
      "hours-per-week  - 5.04%\n",
      "capital-loss    - 3.89%\n",
      "workclass       - 1.73%\n",
      "sex             - 1.38%\n",
      "native-country  - 0.77%\n",
      "race            - 0.48%\n",
      "marital-status  - 0.25%\n"
     ]
    }
   ],
   "source": [
    "feature_importance_data = adult_data.drop([\"income\"], axis=1) \n",
    "feature_importance_data.columns.values\n",
    "\n",
    "# All the features in the correct order.\n",
    "feature_labels = [\"age\", \"workclass\", \"education_num\", \"marital-status\",\n",
    "       \"occupation\", \"relationship\", \"race\",\n",
    "       \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "       \"hours-per-week\", \"native-country\"]\n",
    "\n",
    "importance = gbc_gs.feature_importances_\n",
    "feature_indexes_by_importance = importance.argsort()\n",
    "\n",
    "# Print each feature label, from most importance to least important.\n",
    "print(\"Feature Importance:\\n\")\n",
    "for index in reversed(feature_indexes_by_importance):\n",
    "    importance_percentage = importance[index] * 100.0\n",
    "    print(\"{:<15} - {:.2f}%\".format(feature_labels[index], importance_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19baea67-630b-4005-8444-9c2f156e8039",
   "metadata": {},
   "source": [
    "#### Feature importance quantifies each variable's impact on the model's predictions, helping identify the most relevant features for accuracy.\n",
    "- In our case, it seems that `relationship`, `education_num`, and `capital-gain` is the top three features that affect the target.\n",
    "- It appears that `native-country`, `race`, and `marital-status` have little impact on the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a06874-4a9d-4e64-ad7b-ad9de8a3bcf0",
   "metadata": {},
   "source": [
    "### 6.6. Export Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07afa69e-efb9-4a36-b3f5-d4ae579768e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_adult_income_classifier.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(gbc_gs, \"models/trained_adult_income_classifier.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ddcda3-3bcc-4aa4-92f4-67acf721b380",
   "metadata": {},
   "source": [
    "### 6.7. Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ff55f8c-e1d1-42eb-9b4c-12c90643a96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sample_data = [ # Row 30161, actual `income` => 1.\n",
    "    4,  # `age`\n",
    "    2,\t# `workclass`\n",
    "    0,\t# `education-num`\n",
    "    1,\t# `marital-status`\n",
    "    4,\t# `occupation`\n",
    "    1,\t# `relationship`\n",
    "    1,\t# `race`\n",
    "    1,\t# `sex`\n",
    "    2,\t# `capital-gain`\n",
    "    0,\t# `capital-loss`\n",
    "    2,\t# `hours-per-week`\n",
    "    1,  # `native-country`\n",
    "]\n",
    "\n",
    "sample_data = [sample_data]\n",
    "\n",
    "# Load model.\n",
    "model = joblib.load(\"models/trained_adult_income_classifier.pkl\") \n",
    "\n",
    "# Result.\n",
    "result = model.predict(sample_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71937f-4d61-4a2b-9425-47f9fea96497",
   "metadata": {},
   "source": [
    "#### This sample prediction demonstrates how our exported trained model can be loaded and utilized for making future predictions.\n",
    "\n",
    "Note: The model correctly predicted the sample data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bab33-f790-48f4-bbae-2ee8f7f68e74",
   "metadata": {},
   "source": [
    "### <center>End of Modelling</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
