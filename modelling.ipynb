{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac49463-e1ee-4767-81bb-a2c915847178",
   "metadata": {},
   "source": [
    "### 6.2.0. Library Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "284853cb-3735-4ba4-b84e-5f54f73b3629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Analysis and Wrangling Packages\n",
      "- pandas version: 2.1.4\n",
      "****************************************\n",
      "Modelling Packages\n",
      "- scikit-learn version: 1.3.0\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "def separate():\n",
    "    print('*' * 40)\n",
    "\n",
    "print(\"Data Analysis and Wrangling Packages\")\n",
    "import pandas as pd # Library for data processing and analysis.\n",
    "print(\"- pandas version: {}\". format(pd.__version__))\n",
    "separate()\n",
    "\n",
    "print(\"Modelling Packages\")\n",
    "import sklearn # Collection of machine learning algorithms.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "import joblib # To export model.\n",
    "print(\"- scikit-learn version: {}\". format(sklearn.__version__))\n",
    "\n",
    "# Models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.cluster import AffinityPropagation, Birch, DBSCAN, KMeans\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "separate()\n",
    "\n",
    "# Ignore depreciation warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# Track time taken to run (for modelling).\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c210223-0534-465a-a569-29a0dff44af6",
   "metadata": {},
   "source": [
    "### 6.2.1. Modelling Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d4a931-20cf-40dc-840f-2382c8ac57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquiring data.\n",
    "adult_data = pd.read_csv(\"./data/adult_processed.csv\") \n",
    "\n",
    "# X consists of all the features (target is excluded).\n",
    "X = adult_data.drop([\"income\"], axis=1) \n",
    "\n",
    "# y consists of the target column `income`.\n",
    "y = adult_data[\"income\"] \n",
    "\n",
    "# Splitting the data 7:3.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.3, random_state = 2024) \n",
    "\n",
    "# All 17 machine learning algorithms.\n",
    "model_list = [\n",
    "    (\"BIRCH\", Birch()),\n",
    "    (\"Perceptron\", Perceptron()),\n",
    "    (\"Random Forest\", RandomForestClassifier()),\n",
    "    (\"Decision Tree\", DecisionTreeClassifier()),\n",
    "    (\"Neural Network\", MLPClassifier()),\n",
    "    (\"Adaptive Boosting\", AdaBoostClassifier()),\n",
    "    (\"Gradient Boosting\", GradientBoostingClassifier()),\n",
    "    (\"K-Means Clustering\", KMeans()),\n",
    "    (\"Logistic Regression\", LogisticRegression()),\n",
    "    (\"K-Nearest Neighbours\", KNeighborsClassifier()),\n",
    "    (\"Affinity Propagation\", AffinityPropagation()),\n",
    "    (\"Gaussian Naive Bayes\", GaussianNB()), \n",
    "    (\"Linear Support Vector\", LinearSVC()),  \n",
    "    (\"Bagging Classification\", BaggingClassifier()),\n",
    "    (\"Support Vector Classifier\", SVC()),\n",
    "    (\"Stochastic Gradient Descent\", SGDClassifier()),\n",
    "    (\"Passive Aggressive Classifier\", PassiveAggressiveClassifier()) \n",
    "]\n",
    "\n",
    "# List to store all performances of the models.\n",
    "results = []\n",
    "\n",
    "# Store the name of each model for visualization purposes.\n",
    "model_names = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6fa85a-faf9-4017-9ce9-2ab1419fc5c3",
   "metadata": {},
   "source": [
    "### 6.2.2. Training the 17 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf569cf3-42dd-482b-a3fe-421d54a9cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Time taken: 3783.7183842658997 seconds\n"
     ]
    }
   ],
   "source": [
    "# Start tracking time\n",
    "start_time = time.time()\n",
    "\n",
    "for name, model in model_list:\n",
    "    # Cross-validation, 5 folds.\n",
    "    kfold = KFold(n_splits=5) \n",
    "    # Standard of measure: \"accuracy\" (suitable for this classification problem).\n",
    "    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy') \n",
    "    # Append the mean of 10 folds for each model.\n",
    "    results.append(round(cv_results.mean() * 100, 2)) \n",
    "    # Attach the model name.\n",
    "    model_names.append(name) \n",
    "\n",
    "# Calculate time taken.\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Indication when training is complete.\n",
    "print(f\"Training complete. Time taken: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ceaae9-e366-4035-a2b2-c71dab20a0f0",
   "metadata": {},
   "source": [
    "### 6.2.3. Scores and Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a185a457-dec3-4b2d-87cb-a9e6eaac887c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>84.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adaptive Boosting</td>\n",
       "      <td>84.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Neural Network</td>\n",
       "      <td>84.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Support Vector Classifier</td>\n",
       "      <td>83.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Linear Support Vector</td>\n",
       "      <td>83.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>83.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>83.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bagging Classification</td>\n",
       "      <td>82.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Stochastic Gradient Descent</td>\n",
       "      <td>82.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>82.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>K-Nearest Neighbours</td>\n",
       "      <td>82.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>81.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Passive Aggressive Classifier</td>\n",
       "      <td>79.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>76.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIRCH</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>K-Means Clustering</td>\n",
       "      <td>10.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Affinity Propagation</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Model  Score\n",
       "6               Gradient Boosting  84.26\n",
       "5               Adaptive Boosting  84.21\n",
       "4                  Neural Network  84.07\n",
       "14      Support Vector Classifier  83.91\n",
       "12          Linear Support Vector  83.61\n",
       "8             Logistic Regression  83.58\n",
       "2                   Random Forest  83.05\n",
       "13         Bagging Classification  82.55\n",
       "15    Stochastic Gradient Descent  82.45\n",
       "3                   Decision Tree  82.08\n",
       "9            K-Nearest Neighbours  82.05\n",
       "11           Gaussian Naive Bayes  81.05\n",
       "16  Passive Aggressive Classifier  79.10\n",
       "1                      Perceptron  76.43\n",
       "0                           BIRCH  40.27\n",
       "7              K-Means Clustering  10.19\n",
       "10           Affinity Propagation   0.33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare_models = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'Score': results})\n",
    "compare_models.sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a336d70c-ba21-4538-8078-c60f99bc7f28",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "\n",
    "- Out of 17 models tested, 12 achieved an accuracy of at least 80% on our binary classification task, which is considered exemplary.\n",
    "- Although the Perceptron and Passive Aggressive Classifier performed well, achieving accuracy scores in the 70s range, they fell slightly short of the 80% mark.\n",
    "- The BIRCH model's accuracy of 40% indicates that it technically performs worse than a coin-flip, although it's worth noting that the binary target distribution tends to be skewed toward 0 rather than 1.\n",
    "- The K-Means Clustering and Affinity Propagation models yielded particularly low accuracies, suggesting that these models are not suitable for our specific problem and dataset.\n",
    "\n",
    "#### Due to resource constraints, we will only conduct GridSearch on the best-performing model: Gradient Boosting.\n",
    "\r\n",
    "Note: While we are using GridSearch, the number of iterations will be limited to approximately 400. Consider this a proof-of-concept, but we will select appropriate parameters and ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb01bfd-06e2-472f-b996-ea4f26ed35d2",
   "metadata": {},
   "source": [
    "### 6.3.1. GridSearch Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01127bb-3164-45b7-932d-0f9f7e91e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best performing algorithm. \n",
    "gbc = GradientBoostingClassifier(random_state=2048) \n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [100, 200, 300],  # Number of trees in the forest (higher will increase accuracy, but risk overfitting).\n",
    "    'learning_rate': [0.05, 0.1, 0.2],  # Learning rate (lower requires more trees to capture data relationships but can help prevent overfitting\n",
    "    'max_depth': [3, 4, 5],  # Maximum depth of the trees (Deeper trees can capture more complex patterns but are also more prone to overfitting).\n",
    "    'min_samples_split': [2, 3, 4]  # Minimum number of samples required to split a node (higher results in simpler trees and  less overfitting).\n",
    "}\n",
    "\n",
    "# Number of iterations: 3 (n_estimators) * 3 (learning_rate) * 3 (max_depth) * 3 (min_samples_split) * 5 (CV) = 405 iterations.\n",
    "grid_search = GridSearchCV(estimator=gbc, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe0aed-affa-44e6-bdfe-ea744ce2c89f",
   "metadata": {},
   "source": [
    "### 6.3.2. Running GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dfef0b7-016a-46b4-9506-f9ef4a2b9438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Time taken: 3926.420565843582 seconds\n"
     ]
    }
   ],
   "source": [
    "# Calculate time taken.\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Perform GridSearch.\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Indication when training is complete.\n",
    "print(f\"Training complete. Time taken: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf88a1f-cf35-4d82-ae70-c8b291268a1f",
   "metadata": {},
   "source": [
    "### 6.3.3. GridSearch Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6084008f-a6d5-4b90-b497-1ebaf5ebda9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters:  {'learning_rate': 0.1, 'max_depth': 4, 'min_samples_split': 4, 'n_estimators': 100}\n",
      "Accuracy:  0.8483810365786275\n"
     ]
    }
   ],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters: \", best_params)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb159f32-5931-4867-afea-e2d6dde5abc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66c04b-0795-4217-b3f7-480fe5b0ad4f",
   "metadata": {},
   "source": [
    "### 6.4. Optimal Parameters vs Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a747d978-844c-4282-a146-7b742ce9fd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model:  84.66\n",
      "GridSearch-ed Parameters:  84.96\n"
     ]
    }
   ],
   "source": [
    "# Train a new base Gradient Boosting Model.\n",
    "gbc = GradientBoostingClassifier()\n",
    "gbc.fit(X_train, Y_train)\n",
    "acc_gbc = round(gbc.score(X_train, Y_train) * 100, 2)\n",
    "print(\"Base Model: \", acc_gbc)\n",
    "\n",
    "\n",
    "# Train a new Gradient Boosting Model with the best parameters from GridSearchCV.\n",
    "gbc_gs = GradientBoostingClassifier(learning_rate=0.1, max_depth=4, min_samples_split=4, n_estimators=100)\n",
    "gbc_gs.fit(X_train, Y_train)\n",
    "acc_gbc_gs = round(gbc_gs.score(X_train, Y_train) * 100, 2)\n",
    "print(\"GridSearch-ed Parameters: \", acc_gbc_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9934c746-42d0-4034-9df9-7c2bed397062",
   "metadata": {},
   "source": [
    "#### YAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a06874-4a9d-4e64-ad7b-ad9de8a3bcf0",
   "metadata": {},
   "source": [
    "### 6.5. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07afa69e-efb9-4a36-b3f5-d4ae579768e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/trained_adult_income_classifier.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(gbc_gs, \"models/trained_adult_income_classifier.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ddcda3-3bcc-4aa4-92f4-67acf721b380",
   "metadata": {},
   "source": [
    "### 6.Sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ff55f8c-e1d1-42eb-9b4c-12c90643a96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "sample_data = [ # Row 30161, actual `income` => 1.\n",
    "    4, # `age`\n",
    "    2,\t# `workclass`\n",
    "    0,\t# `education-num`\n",
    "    1,\t# `marital-status`\n",
    "    4,\t# `occupation`\n",
    "    1,\t# `relationship`\n",
    "    1,\t# `race`\n",
    "    1,\t# `sex`\n",
    "    2,\t# `capital-gain`\n",
    "    0,\t# `capital-loss`\n",
    "    2,\t# `hours-per-week`\n",
    "    1,   # `native-country`\n",
    "]\n",
    "\n",
    "sample_data = [sample_data]\n",
    "\n",
    "# Load model.\n",
    "model = joblib.load('models/trained_adult_income_classifier.pkl') \n",
    "\n",
    "# Result.\n",
    "result = model.predict(sample_data)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8bab33-f790-48f4-bbae-2ee8f7f68e74",
   "metadata": {},
   "source": [
    "### End of Modelling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
